print("Logistic Regression with 2-fold Cross Validation 3\n")
coda(t0, model, "glm", valid, response=y, varimp)
}
all.fit <- function(fitMethod, predictors, response, train, valid) { fitMethod(predictors, response, train, valid) }
#iterate over the fit fcns
model.fit.fcns <- c(lr.fit, rf.fit) #, gbm.fit, dl.fit)
models <- unlist(recursive = F, lapply(model.fit.fcns, all.fit, predictors, response, train, valid))
lr.fit <- function(x, y, train, valid) {
print("Logistic Regression with 2-fold Cross Validation\n", y)
t0 <- Sys.time()
model <- h2o.glm(x=x, y=y, data=train, family="binomial", nfolds=2, variable_importances=TRUE)
varimp <- paste(names(sort(model@model$coefficients, T))[1:length(x)], collapse = ",", sep = ",")
coda(t0, model, "glm", valid, response=y, varimp)
}
model.fit.fcns <- c(lr.fit, rf.fit) #, gbm.fit, dl.fit)
models <- unlist(recursive = F, lapply(model.fit.fcns, all.fit, predictors, response, train, valid))
model.fit.fcns <- c(lr.fit) #, rf.fit) #, gbm.fit, dl.fit)
models <- unlist(recursive = F, lapply(model.fit.fcns, all.fit, predictors, response, train, valid))
lr.fit <- function(x, y, train, valid) {
print("Logistic Regression with 2-fold Cross Validation\n", y)
t0 <- Sys.time()
model <- h2o.glm(x=x, y=y, data=train, family="binomial", nfolds=2, variable_importances=TRUE)
varimp <- paste(names(sort(model@model$coefficients, T))[1:length(x)], collapse = ",", sep = ",")
coda(t0, model, "glm", valid, response=y, varimp)
}
all.fit <- function(fitMethod, predictors, response, train, valid) { fitMethod(predictors, response, train, valid) }
#iterate over the fit fcns
model.fit.fcns <- c(lr.fit) #, rf.fit) #, gbm.fit, dl.fit)
models <- unlist(recursive = F, lapply(model.fit.fcns, all.fit, predictors, response, train, valid))
models <- unlist(recursive = F, lapply(model.fit.fcns, all.fit, low_level_predictors, response, train, valid))
models <- unlist(recursive = F, lapply(model.fit.fcns, all.fit, c(4,5), response, train, valid))
q()
library(h2o)
h2oServer <- h2o.init()
homedir <- paste0(path.expand("~"),"/Higgs/") #modify if needed
TRAIN = "HIGGS.100k.csv.gz"
data_hex <- h2o.importFile(h2oServer, path = paste0(homedir,TRAIN), header = F, sep = ',', key = 'train.hex')
random <- h2o.runif(train_hex, seed = 123456789)
train_hex <- data_hex[random < .6,]
valid_hex <- data_hex[random >= .6 & random < .8,]
test_hex  <- data_hex[random >= .8,]
low_level_predictors = c(2:21)
high_level_predictors = c(22:28)
response = 1
get_auc <- function(model, data, response) {
pred <- h2o.predict(model, data)[,3]
perf <- h2o.performance(pred, data[,response])
perf@model$auc
}
coda <- function(t0, model, modeltype, validation, response, varimp) {
elapsed_seconds <- as.numeric(Sys.time() - t0)
modelkey <- model@key
type <- modeltype
#perform the holdout computation
auc <- get_auc(model, validation, response)
result <- list(list(model, modeltype, response, elapsed_seconds, auc, varimp))
names(result) <- model@key
return(result)
}
lr.fit <- function(x, y, train, valid) {
print("Logistic Regression with 2-fold Cross Validation\n", y)
t0 <- Sys.time()
model <- h2o.glm(x=x, y=y, data=train, family="binomial", nfolds=2, variable_importances=TRUE)
varimp <- paste(names(sort(model@model$coefficients, T))[1:length(x)], collapse = ",", sep = ",")
coda(t0, model, "glm", valid, response=y, varimp)
}
all.fit <- function(fitMethod, predictors, response, train, valid) { fitMethod(predictors, response, train, valid) }
model.fit.fcns <- c(lr.fit) #, rf.fit) #, gbm.fit, dl.fit)
models <- unlist(recursive = F, lapply(model.fit.fcns, all.fit, c(4,5), response, train, valid))
get_auc <- function(model, data, response) {
pred <- h2o.predict(model, data)[,3]
perf <- h2o.performance(pred, data[,response])
perf@model$auc
}
validate <- function(t0, model, modeltype, validation, response, varimp) {
elapsed_seconds <- as.numeric(Sys.time() - t0)
modelkey <- model@key
type <- modeltype
auc <- get_auc(model, validation, response)
result <- list(list(model, modeltype, response, elapsed_seconds, auc, varimp))
names(result) <- model@key
return(result)
}
lr.fit <- function(x, y, train, valid) {
print("Logistic Regression with 2-fold Cross Validation\n", y)
t0 <- Sys.time()
model <- h2o.glm(x=x, y=y, data=train, family="binomial", nfolds=2, variable_importances=TRUE)
varimp <- paste(names(sort(model@model$coefficients, T))[1:length(x)], collapse = ",", sep = ",")
validate(t0, model, "glm", valid, response=y, varimp)
}
rf.fit <- function(x, y, train, valid) {
print("Random Forest with 2-fold Cross Validation\n")
t0 <- Sys.time()
model <-h2o.randomForest(x, y, data=train, nfolds=2, importances=TRUE, ntree=c(10,20), ndepth=c(10,20))
varimp <- paste(names(sort(model@model$varimp[1,]))[1:NUM_FEATURES], collapse = ",", sep = ",")
validate(t0, model, "rf", valid, response=y, varimp)
}
gbm.fit <- function(x, y, train, valid, test) {
h2o.gbm(x, y, data=train, validation=valid)
}
dl.fit <- function(x, y, data) {
h2o.deeplearning(x, y, data=train, validation=valid)
}
all.fit <- function(fitMethod, predictors, response, train, valid) { fitMethod(predictors, response, train, valid) }
model.fit.fcns <- c(lr.fit) #, rf.fit) #, gbm.fit, dl.fit)
models <- unlist(recursive = F, lapply(model.fit.fcns, all.fit, low_level_predictors, response, train, valid))
models <- unlist(recursive = F, lapply(model.fit.fcns, all.fit, low_level_predictors, response, train_hex, valid_hex))
models <- unlist(recursive = F, lapply(model.fit.fcns, all.fit, low_level_predictors, response, train_hex, valid_hex))
?h2o.glm
train_hex
train_hex <- data_hex[random < .6,]
library(h2o)
h2oServer <- h2o.init()
homedir <- paste0(path.expand("~"),"/Higgs/") #modify if needed
TRAIN = "HIGGS.100k.csv.gz"
data_hex <- h2o.importFile(h2oServer, path = paste0(homedir,TRAIN), header = F, sep = ',', key = 'train.hex')
######We split the dataset into 3 pieces. Training will be done on random 60%, parameter tuning and model selection will be done on the 20% validation holdout, and final model testing is done on the remaining 20%.
random <- h2o.runif(train_hex, seed = 123456789)
train_hex <- data_hex[random < .6,]
valid_hex <- data_hex[random >= .6 & random < .8,]
test_hex  <- data_hex[random >= .8,]
low_level_predictors = c(2:21)
high_level_predictors = c(22:28)
response = 1
library(h2o)
h2oServer <- h2o.init(nthreads=-1)
h2o.shutdown()
h2o.shutdown(h2oServefr)
h2o.shutdown(h2oServer)
library(h2o)
h2oServer <- h2o.init(nthreads=-1)
homedir <- paste0(path.expand("~"),"/Higgs/") #modify if needed
TRAIN = "HIGGS.100k.csv.gz"
data_hex <- h2o.importFile(h2oServer, path = paste0(homedir,TRAIN), header = F, sep = ',', key = 'train.hex')
######We split the dataset into 3 pieces. Training will be done on random 60%, parameter tuning and model selection will be done on the 20% validation holdout, and final model testing is done on the remaining 20%.
random <- h2o.runif(train_hex, seed = 123456789)
train_hex <- data_hex[random < .6,]
valid_hex <- data_hex[random >= .6 & random < .8,]
test_hex  <- data_hex[random >= .8,]
low_level_predictors = c(2:21)
high_level_predictors = c(22:28)
response = 1
get_auc <- function(model, data, response) {
pred <- h2o.predict(model, data)[,3]
perf <- h2o.performance(pred, data[,response])
perf@model$auc
}
validate <- function(t0, model, modeltype, validation, response, varimp) {
elapsed_seconds <- as.numeric(Sys.time() - t0)
modelkey <- model@key
type <- modeltype
auc <- get_auc(model, validation, response)
result <- list(list(model, modeltype, response, elapsed_seconds, auc, varimp))
names(result) <- model@key
return(result)
}
lr.fit <- function(x, y, train, valid) {
print("Logistic Regression with 2-fold Cross Validation\n", y)
t0 <- Sys.time()
model <- h2o.glm(x=x, y=y, data=train, family="binomial", nfolds=2, variable_importances=TRUE)
varimp <- paste(names(sort(model@model$coefficients, T))[1:length(x)], collapse = ",", sep = ",")
validate(t0, model, "glm", valid, response=y, varimp)
}
rf.fit <- function(x, y, train, valid) {
print("Random Forest with 2-fold Cross Validation\n")
t0 <- Sys.time()
model <-h2o.randomForest(x, y, data=train, nfolds=2, importances=TRUE, ntree=c(10,20), ndepth=c(10,20))
varimp <- paste(names(sort(model@model$varimp[1,]))[1:NUM_FEATURES], collapse = ",", sep = ",")
validate(t0, model, "rf", valid, response=y, varimp)
}
gbm.fit <- function(x, y, train, valid, test) {
h2o.gbm(x, y, data=train, validation=valid)
}
dl.fit <- function(x, y, data) {
h2o.deeplearning(x, y, data=train, validation=valid)
}
all.fit <- function(fitMethod, predictors, response, train, valid) { fitMethod(predictors, response, train, valid) }
model.fit.fcns <- c(lr.fit) #, rf.fit) #, gbm.fit, dl.fit)
models <- unlist(recursive = F, lapply(model.fit.fcns, all.fit, low_level_predictors, response, train_hex, valid_hex))
train_hex
random
random <- h2o.runif(train_hex, seed = 123456789)
data_hex <- h2o.importFile(h2oServer, path = paste0(homedir,TRAIN), header = F, sep = ',', key = 'train.hex')
random <- h2o.runif(data_hex, seed = 123456789)
train_hex <- data_hex[random < .6,]
valid_hex <- data_hex[random >= .6 & random < .8,]
test_hex  <- data_hex[random >= .8,]
low_level_predictors = c(2:21)
high_level_predictors = c(22:28)
response = 1
get_auc <- function(model, data, response) {
pred <- h2o.predict(model, data)[,3]
perf <- h2o.performance(pred, data[,response])
perf@model$auc
}
validate <- function(t0, model, modeltype, validation, response, varimp) {
elapsed_seconds <- as.numeric(Sys.time() - t0)
modelkey <- model@key
type <- modeltype
auc <- get_auc(model, validation, response)
result <- list(list(model, modeltype, response, elapsed_seconds, auc, varimp))
names(result) <- model@key
return(result)
}
lr.fit <- function(x, y, train, valid) {
print("Logistic Regression with 2-fold Cross Validation\n", y)
t0 <- Sys.time()
model <- h2o.glm(x=x, y=y, data=train, family="binomial", nfolds=2, variable_importances=TRUE)
varimp <- paste(names(sort(model@model$coefficients, T))[1:length(x)], collapse = ",", sep = ",")
validate(t0, model, "glm", valid, response=y, varimp)
}
rf.fit <- function(x, y, train, valid) {
print("Random Forest with 2-fold Cross Validation\n")
t0 <- Sys.time()
model <-h2o.randomForest(x, y, data=train, nfolds=2, importances=TRUE, ntree=c(10,20), ndepth=c(10,20))
varimp <- paste(names(sort(model@model$varimp[1,]))[1:NUM_FEATURES], collapse = ",", sep = ",")
validate(t0, model, "rf", valid, response=y, varimp)
}
gbm.fit <- function(x, y, train, valid, test) {
h2o.gbm(x, y, data=train, validation=valid)
}
dl.fit <- function(x, y, data) {
h2o.deeplearning(x, y, data=train, validation=valid)
}
all.fit <- function(fitMethod, predictors, response, train, valid) { fitMethod(predictors, response, train, valid) }
model.fit.fcns <- c(lr.fit) #, rf.fit) #, gbm.fit, dl.fit)
models <- unlist(recursive = F, lapply(model.fit.fcns, all.fit, low_level_predictors, response, train_hex, valid_hex))
train_hex
data_hex <- h2o.importFile(h2oServer, path = paste0(homedir,TRAIN), header = F, sep = ',', key = 'data_hex')
random <- h2o.runif(data_hex, seed = 123456789)
train_hex <- data_hex[random < .6,]
valid_hex <- data_hex[random >= .6 & random < .8,]
test_hex  <- data_hex[random >= .8,]
train_hex
low_level_predictors = c(2:21)
high_level_predictors = c(22:28)
response = 1
get_auc <- function(model, data, response) {
pred <- h2o.predict(model, data)[,3]
perf <- h2o.performance(pred, data[,response])
perf@model$auc
}
validate <- function(t0, model, modeltype, validation, response, varimp) {
elapsed_seconds <- as.numeric(Sys.time() - t0)
modelkey <- model@key
type <- modeltype
auc <- get_auc(model, validation, response)
result <- list(list(model, modeltype, response, elapsed_seconds, auc, varimp))
names(result) <- model@key
return(result)
}
lr.fit <- function(x, y, train, valid) {
print("Logistic Regression with 2-fold Cross Validation\n", y)
t0 <- Sys.time()
model <- h2o.glm(x=x, y=y, data=train, family="binomial", nfolds=2, variable_importances=TRUE)
varimp <- paste(names(sort(model@model$coefficients, T))[1:length(x)], collapse = ",", sep = ",")
validate(t0, model, "glm", valid, response=y, varimp)
}
rf.fit <- function(x, y, train, valid) {
print("Random Forest with 2-fold Cross Validation\n")
t0 <- Sys.time()
model <-h2o.randomForest(x, y, data=train, nfolds=2, importances=TRUE, ntree=c(10,20), ndepth=c(10,20))
varimp <- paste(names(sort(model@model$varimp[1,]))[1:NUM_FEATURES], collapse = ",", sep = ",")
validate(t0, model, "rf", valid, response=y, varimp)
}
gbm.fit <- function(x, y, train, valid, test) {
h2o.gbm(x, y, data=train, validation=valid)
}
dl.fit <- function(x, y, data) {
h2o.deeplearning(x, y, data=train, validation=valid)
}
all.fit <- function(fitMethod, predictors, response, train, valid) { fitMethod(predictors, response, train, valid) }
model.fit.fcns <- c(lr.fit) #, rf.fit) #, gbm.fit, dl.fit)
train_hex
library(h2o)
h2oServer <- h2o.init(nthreads=-1)
homedir <- paste0(path.expand("~"),"/Higgs/") #modify if needed
TRAIN = "HIGGS.100k.csv.gz"
data_hex <- h2o.importFile(h2oServer, path = paste0(homedir,TRAIN), header = F, sep = ',', key = 'data_hex')
random <- h2o.runif(data_hex, seed = 123456789)
train_hex <- data_hex[random < .6,]
valid_hex <- data_hex[random >= .6 & random < .8,]
test_hex  <- data_hex[random >= .8,]
low_level_predictors = c(2:21)
high_level_predictors = c(22:28)
response = 1
train_hex
get_auc <- function(model, data, response) {
pred <- h2o.predict(model, data)[,3]
perf <- h2o.performance(pred, data[,response])
perf@model$auc
}
train_hex
validate <- function(t0, model, modeltype, validation, response, varimp) {
elapsed_seconds <- as.numeric(Sys.time() - t0)
modelkey <- model@key
type <- modeltype
auc <- get_auc(model, validation, response)
result <- list(list(model, modeltype, response, elapsed_seconds, auc, varimp))
names(result) <- model@key
return(result)
}
train_hex
lr.fit <- function(x, y, train, valid) {
print("Logistic Regression with 2-fold Cross Validation\n", y)
t0 <- Sys.time()
model <- h2o.glm(x=x, y=y, data=train, family="binomial", nfolds=2, variable_importances=TRUE)
varimp <- paste(names(sort(model@model$coefficients, T))[1:length(x)], collapse = ",", sep = ",")
validate(t0, model, "glm", valid, response=y, varimp)
}
train_hex
all.fit <- function(fitMethod, predictors, response, train, valid) { fitMethod(predictors, response, train, valid) }
train_hex
model.fit.fcns <- c(lr.fit) #, rf.fit) #, gbm.fit, dl.fit)
train_hex
models <- unlist(recursive = F, lapply(model.fit.fcns, all.fit, low_level_predictors, response, train_hex, valid_hex))
lr.fit <- function(x, y, train, valid) {
print("Logistic Regression with 2-fold Cross Validation\n", y)
t0 <- Sys.time()
model <- h2o.glm(x=x, y=y, data=train, family="binomial", nfolds=2, variable_importances=TRUE, use_all_factor_levels=TRUE)
varimp <- paste(names(sort(model@model$coefficients, T))[1:length(x)], collapse = ",", sep = ",")
validate(t0, model, "glm", valid, response=y, varimp)
}
models <- unlist(recursive = F, lapply(model.fit.fcns, all.fit, low_level_predictors, response, train_hex, valid_hex))
lr.fit <- function(x, y, train, valid) {
print("Logistic Regression with 2-fold Cross Validation\n", y)
t0 <- Sys.time()
model <- h2o.glm(x=x, y=y, data=train, family="binomial", nfolds=2, variable_importances=TRUE, use_all_factor_levels=TRUE)
varimp <- paste(names(sort(model@model$coefficients, T))[1:length(x)], collapse = ",", sep = ",")
validate(t0, model, "glm", valid, response=y, varimp)
}
all.fit <- function(fitMethod, predictors, response, train, valid) { fitMethod(predictors, response, train, valid) }
model.fit.fcns <- c(lr.fit) #, rf.fit) #, gbm.fit, dl.fit)
models <- unlist(recursive = F, lapply(model.fit.fcns, all.fit, low_level_predictors, response, train_hex, valid_hex))
models.auc.response.frame <- as.data.frame(t(as.data.frame(lapply(models, selectModel))))
selectModel <- function(x) {
c(model_key = x[[1]]@key,
model_type = x[[2]],
train_auc = as.numeric(x[[1]]@model$auc),
validation_auc = as.numeric(x[[5]]),
important_feat = x[[6]],
response = x[[3]],
train_time_s = as.numeric(as.character(x[[4]])))
}
models.auc.response.frame <- as.data.frame(t(as.data.frame(lapply(models, selectModel))))
models.auc.response.frame$train_auc <- as.numeric(as.character(models.auc.response.frame$train_auc))
models.auc.response.frame$validation_auc <- as.numeric(as.character(models.auc.response.frame$validation_auc))
# sort the models by auc from worst to best
models.sort.by.auc <- models.auc.response.frame[with(models.auc.response.frame, order(response, validation_auc)),-1]
models.sort.by.auc <- models.sort.by.auc[rev(rownames(models.sort.by.auc)),]
# convert the `auc` and `train_time` columns into numerics
models.sort.by.auc$train_auc       <- as.numeric(as.character(models.sort.by.auc$train_auc))
models.sort.by.auc$validation_auc  <- as.numeric(as.character(models.sort.by.auc$validation_auc))
models.sort.by.auc$train_time      <- as.numeric(as.character(models.sort.by.auc$train_time))
# display the frame
print(models.sort.by.auc)
print(models.sort.by.auc)
best_model <- h2o.getModel(h, rownames(models.sort.by.auc)[1])
best_model <- h2o.getModel(h2oServer, rownames(models.sort.by.auc)[1])
test_auc <- test_performance(best_model, test, Delayed)  # Swap out test to any datset to do the final scoring on.
test_auc <- get_auc(best_model, test, Delayed)  # Swap out test to any datset to do the final scoring on.
test_auc <- validate(best_model, test, Delayed)  # Swap out test to any datset to do the final scoring on.
test_auc <- validate(best_model, test_hex, response)  # Swap out test to any datset to do the final scoring on.
best_model <- h2o.getModel(h2oServer, rownames(models.sort.by.auc)[1])
test_auc <- validate(best_model, test_hex, response)  # Swap out test to any datset to do the final scoring on.
best_model <- h2o.getModel(h2oServer, rownames(models.sort.by.auc)[1])
best_model
cat(paste(" -------------------------------\n",
"Best Model Performance On Final Testing Data:", "\n",
"AUC = ", test_auc, "\n",
"--------------------------------\n"))
test_auc <- validate(best_model, test_hex, response)
print(models.sort.by.auc)
preds <- h2o.predict(best_model, test_hex)
test_auc <- validate(best_model, test_hex, response)
cat(paste(" -------------------------------\n",
"Best Model Performance On Final Testing Data:", "\n",
"AUC = ", test_auc, "\n",
"--------------------------------\n"))
test_auc <- validate(best_model, test_hex, response)
cat(paste(" -------------------------------\n",
"Best Model Performance On Final Testing Data:", "\n",
"AUC = ", test_auc, "\n",
"--------------------------------\n"))
best_model <- h2o.getModel(h2oServer, rownames(models.sort.by.auc)[1])
preds <- h2o.predict(best_model, test_hex)
test_auc <- validate(best_model, test_hex, response)
best_model
test_hex
response
test_auc <- validate(best_model, test_hex, response)
q()
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
install.packages("h2o", repos=(c("file:///Users/arno/h2o/target/R", getOption("repos"))))
library(h2o)
h2oServer <- h2o.init()
homedir <- paste0(path.expand("~"),"/h2o/") #modify if needed
TRAIN = "smalldata/adult.gz"
data_hex <- h2o.importFile(h2oServer, path = paste0(homedir,TRAIN), header = F, sep = ' ', key = 'data_hex')
######We manually assign column names since they are missing in the original file
colnames(data_hex) <- c("age","workclass","fnlwgt","education","education-num","marital-status","occupation","relationship","race","sex","capital-gain","capital-loss","hours-per-week","native-country","income")
summary(data_hex)
######We will try to predict whether the income is more or less than $50k.
summary(data_hex$income)
response = "income"
####First, we source some helper code that allows us to compare models.
setwd("~/h2o-training/tutorials/advanced/adult")
source("../helper.R")
###### We then add this simple helper function to split a frame into train/valid/test pieces, train a GLM and a GBM model with 2-fold cross-validation and obtaining the best model after printing a leaderbaord.
N_FOLDS = 2
h2o.trainModels <- function(frame) {
# split the data into train/valid/test
random <- h2o.runif(frame, seed = 123456789)
train_hex <- h2o.assign(frame[random < .8,], "train_hex")
valid_hex <- h2o.assign(frame[random >= .8 & random < .9,], "valid_hex")
test_hex  <- h2o.assign(frame[random >= .9,], "test_hex")
predictors <- colnames(frame)[-match(response,colnames(frame))]
# multi-model comparison with N-fold cross-validation
data = list(x=predictors, y=response, train=train_hex, valid=valid_hex, nfolds=N_FOLDS)
models <- c(
h2o.fit(h2o.glm, data, glmparams),
h2o.fit(h2o.gbm, data, gbmparams)
)
best_model <- h2o.leaderBoard(models, test_hex, match(response,colnames(frame)))
h2o.rm(h2oServer, grep(pattern = "Last.value", x = h2o.ls(h2oServer)$Key, value = TRUE))
best_model
}
### Baseline performance on original dataset
###### For simplicity, we use default parameters (no grid search parameter tuning) to establish baseline performance numbers for this dataset.
glmparams <- list(family="binomial", variable_importances=T, use_all_factor_levels=T)
gbmparams <- list(importance=TRUE)
glmparams <- list(family="binomial", variable_importances=F, lambda=1e-5, alpha=c(0.01,0.25,0.5,0.75,0.99), higher_accuracy=T, use_all_factor_levels=T)
###glmparams <- list(family="binomial", variable_importances=F, lambda=1e-5, alpha=0.3, higher_accuracy=T, use_all_factor_levels=T)
best_model <- h2o.trainModels(data_hex)
glmparams <- list(family="binomial", variable_importances=F, lambda=c(1e-5,1e-4), alpha=c(0.01,0.25,0.5,0.75,0.99), higher_accuracy=T, use_all_factor_levels=T)
best_model <- h2o.trainModels(data_hex)
glmparams <- list(family="binomial", variable_importances=F, alpha=c(0.01,0.25,0.5,0.75,0.99), higher_accuracy=T, use_all_factor_levels=T)
best_model <- h2o.trainModels(data_hex)
glmparams <- list(family="binomial", variable_importances=T, use_all_factor_levels=T)
gbmparams <- list(importance=TRUE)
best_model <- h2o.trainModels(data_hex)
h2o.append <- function(frame, col) {
appended_frame <- h2o.assign(cbind(frame, col), "appended_frame")
appended_frame
}
####1. Turn age into a factor
######The feature `age` is an integer value, but GLM for example will have a tough time predicing income from age with a linear relationship, while GBM should be able to carve out these non-linear dependencies by itself (but it might need more trees, deeper interaction depth than default values)
data_hex <- h2o.append(data_hex, as.factor(data_hex$age))
colnames(data_hex)
summary(data_hex)
best_model <- h2o.trainModels(data_hex)
head(sort(best_model@model$normalized_coefficients,decreasing=T),5)
###### And now the largest negatively correlated coefficients (surprise, surprise: young people don't eary money):
head(sort(best_model@model$normalized_coefficients,decreasing=F),5)
head(sort(best_model@model$normalized_coefficients,decreasing=T),5)
head(sort(best_model@model$normalized_coefficients,decreasing=F),5)
data_hex <- h2o.append(data_hex, as.factor(data_hex$'hours-per-week'))
data_hex <- h2o.append(data_hex, as.factor(data_hex$'capital-gain'))
data_hex <- h2o.append(data_hex, as.factor(data_hex$'capital-loss'))
colnames(data_hex)
summary(data_hex)
best_model <- h2o.trainModels(data_hex)
###### Let's give GBM a shot at beating GLM by using better parameters:
gbmparams <- list(importance=TRUE, n.tree=50, interaction.depth=10)
best_model <- h2o.trainModels(data_hex)
factor_interactions <- h2o.interaction(data_hex, factors = c("marital-status","relationship","education"), pairwise = TRUE, max_factors = 1000, min_occurrence = 1)
summary(factor_interactions)
data_hex <- h2o.append(data_hex, factor_interactions)
colnames(data_hex)
summary(data_hex)
best_model <- h2o.trainModels(data_hex)
######This tutorial shows how a Deep Learning [Auto-Encoder](http://en.wikipedia.org/wiki/Autoencoder) model can be used to find outliers in a dataset. This file is both valid R and markdown code. We use the well-known [MNIST](http://yann.lecun.com/exdb/mnist/) dataset of hand-written digits, where each row contains the 28^2=784 raw gray-scale pixel values from 0 to 255 of the digitized digits (0 to 9).
### Start H2O and load the MNIST data
######Initialize the H2O server and import the MNIST training/testing datasets.
library(h2o)
h2oServer <- h2o.init()
homedir <- paste0(path.expand("~"),"/h2o/") #modify if needed
TRAIN = "smalldata/mnist/train.csv.gz"
TEST = "smalldata/mnist/test.csv.gz"
train_hex <- h2o.importFile(h2oServer, path = paste0(homedir,TRAIN), header = F, sep = ',', key = 'train.hex')
test_hex <- h2o.importFile(h2oServer, path = paste0(homedir,TEST), header = F, sep = ',', key = 'test.hex')
######The data consists of 784 (=28^2) pixel values per row, with (gray-scale) values from 0 to 255. The last column is the response (a label in 0,1,2,...,9).
predictors = c(1:784)
resp = 785
######We do unsupervised training, so we can drop the response column.
train_hex <- train_hex[,-resp]
test_hex <- test_hex[,-resp]
### Finding outliers - ugly hand-written digits
######We train a Deep Learning Auto-Encoder to learn a compressed (low-dimensional) non-linear representation of the dataset, hence learning the intrinsic structure of the training dataset. The auto-encoder model is then used to transform all test set images to their reconstructed images, by passing through the lower-dimensional neural network. We then find outliers in a test dataset by comparing the reconstruction of each scanned digit with its original pixel values. The idea is that a high reconstruction error of a digit indicates that the test set point doesn't conform to the structure of the training data and can hence be called an outlier.
####1. Learn what's *normal* from the training data
######Train unsupervised Deep Learning autoencoder model on the training dataset. For simplicity, we train a model with 1 hidden layer of 50 Tanh neurons (should be less than 784 for it to compress), and train for 1 epoch (one pass over the data). We explicitly include constant columns (all white background) for the visualization to be easier.
ae_model <- h2o.deeplearning(x=predictors,
y=42, #response (ignored - pick any non-constant column)
data=train_hex,
activation="Tanh",
autoencoder=T,
hidden=c(50),
ignore_const_cols=F,
epochs=1)
######Note that the response column is ignored (it is only required because of a shared DeepLearning code framework).
####2. Find outliers in the test data
######The Anomaly app computes the per-row reconstruction error for the test data set. It passes it through the autoencoder model (built on the training data) and computes mean square error (MSE) for each row in the test set.
test_rec_error <- as.data.frame(h2o.anomaly(test_hex, ae_model))
######In case you wanted to see the lower-dimensional features created by the auto-encoder deep learning model, here's a way to extract them for a given dataset:
low_dimensional_features <- h2o.deepfeatures(test_hex, ae_model, layer=1)
summary(low_dimensional_features)
####3. Visualize the *good*, the *bad* and the *ugly*
summary(low_dimensional_features)
low_dimensional_features_PCA <- h2o.prcomp
####3. Visualize the *good*, the *bad* and the *ugly*
low_dimensional_features_PCA <- h2o.prcomp(test_hex, 10)
